{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/scavenger/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  \\\n",
      "3   commissar walking slowly passed badly disguise...   \n",
      "6   let epitaph born nameless abandoned gutter sun...   \n",
      "7   commissar must cold sweat walking line krieger...   \n",
      "8   3 krieg commandment 1if get hit bullet destroy...   \n",
      "9   man men one pebble avalanche one drop flood li...   \n",
      "12  god song pain find every 2 week mind automatic...   \n",
      "15                sothe enemy u surrounded great miss   \n",
      "17  imperium basically communism fascism absolute ...   \n",
      "19                  cant believe army worth 500 point   \n",
      "20             glory first man die commissar dawn war   \n",
      "22    life war death peace life shame death atonement   \n",
      "25  one lasgun shoot one without follows one lasgu...   \n",
      "30  service life death emperor debt blood shall pa...   \n",
      "34  played stellaris got attacked great khan becas...   \n",
      "35  ironic thing russian know song dictatorial reg...   \n",
      "37  lyric en shackled together one chain forced pu...   \n",
      "40  general im starting get tired war sending youn...   \n",
      "46  death barrier call call arm call kill enemy ma...   \n",
      "49  death merciful life painful time fleeting trut...   \n",
      "51                                  shame die emperor   \n",
      "53              death korp like regular imperium army   \n",
      "55  child emperor wall terra heretic cadia must fa...   \n",
      "\n",
      "                           author                  time Sentiment  \n",
      "3               @kevinchester5217            1 year ago  Negative  \n",
      "6                @techypriest7523            1 year ago  Negative  \n",
      "7                @thegeneralmitch           2 years ago  Negative  \n",
      "8              @blockwithaglock96           2 years ago  Negative  \n",
      "9                  @ToozdaysChild           2 years ago  Negative  \n",
      "12                 @KingMagsarion  2 years ago (edited)  Negative  \n",
      "15                  @quiducle8825            1 year ago  Negative  \n",
      "17                       @Orthane           2 years ago  Negative  \n",
      "19                  @ducklord2000           2 years ago  Negative  \n",
      "20                 @vajdamark2209            1 year ago  Negative  \n",
      "22                  @johnsnow6602           2 years ago  Negative  \n",
      "25               @vincentrees4970           2 years ago  Negative  \n",
      "30                       @159tony            1 year ago  Negative  \n",
      "34                      @bruh1740            1 year ago  Negative  \n",
      "35                @hanspanzer1260           2 years ago  Negative  \n",
      "37                 @lunaflora7542  2 years ago (edited)  Negative  \n",
      "40                @matebencze8546           2 years ago  Negative  \n",
      "46               @lastsoldier4524            1 year ago  Negative  \n",
      "49                    @buddha6932         11 months ago  Negative  \n",
      "51  @hurrikanlouitrollthemall1241          9 months ago  Negative  \n",
      "53                @kebabkebob7808           2 years ago  Negative  \n",
      "55           @grakalamontages5066            1 year ago  Negative  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Chrome\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "\n",
    "def process_youtube_comments(youtube_video_url, category):\n",
    "    data = []\n",
    "    user = []\n",
    "    timer = []\n",
    "\n",
    "    chromedriver_autoinstaller.install()\n",
    "    with Chrome() as driver:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        driver.get(youtube_video_url)\n",
    "\n",
    "        for item in range(4):  # By increasing the highest range you can get more content\n",
    "            wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "\n",
    "        for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#comment #content-text\"))):\n",
    "            data.append(comment.text)\n",
    "\n",
    "        for author in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#header-author #author-text\"))):\n",
    "            user.append(author.text)\n",
    "\n",
    "        for times in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#header-author #published-time-text\"))):\n",
    "            timer.append(times.text)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['comment'])\n",
    "    df_ = pd.DataFrame(user, columns=['author'])\n",
    "    df_time = pd.DataFrame(timer, columns=['time'])\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.concat([df, df_, df_time], axis=1).dropna()\n",
    "    \n",
    "    # Language settings\n",
    "    idioma = \"english\"\n",
    "    stop_words = stopwords.words(idioma)\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    snowball_stemmer = SnowballStemmer(language=idioma)\n",
    "    lzr = WordNetLemmatizer()\n",
    "\n",
    "    def text_processing(text):\n",
    "        # Convierte todo en string\n",
    "        text = str(text)\n",
    "        \n",
    "        # Conver text in lower\n",
    "        text = text.lower()\n",
    "\n",
    "        # remove new line characters in text\n",
    "        text = re.sub(r'\\n',' ', text)\n",
    "        \n",
    "        # remove punctuations from text\n",
    "        text = re.sub('[%s]' % re.escape(punctuation), \"\", text)\n",
    "        \n",
    "        # remove references and hashtags from text\n",
    "        text = re.sub(\"^a-zA-Z0-9$,.\", \"\", text)\n",
    "        \n",
    "        # remove multiple spaces from text\n",
    "        text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "        \n",
    "        # remove special characters from text\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "        text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n",
    "        \n",
    "        # lemmatizer using WordNetLemmatizer from nltk package\n",
    "        text = ' '.join([lzr.lemmatize(word) for word in word_tokenize(text)])\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    combined_df['comment'] = combined_df['comment'].apply(text_processing)\n",
    "\n",
    "    # Sentiment analysis\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    combined_df[\"Positive\"] = [sentiment.polarity_scores(i)[\"pos\"] for i in combined_df['comment']]\n",
    "    combined_df[\"Negative\"] = [sentiment.polarity_scores(i)[\"neg\"] for i in combined_df['comment']]\n",
    "    combined_df[\"Neutral\"] = [sentiment.polarity_scores(i)[\"neu\"] for i in combined_df['comment']]\n",
    "    combined_df[\"Compound\"] = [sentiment.polarity_scores(i)[\"compound\"] for i in combined_df['comment']]\n",
    "\n",
    "    neg = combined_df[\"Negative\"].values\n",
    "    pos = combined_df[\"Positive\"].values\n",
    "\n",
    "    # Determinar el sentimiento basado en las puntuaciones de negativo y positivo\n",
    "    sentiments = []\n",
    "    for n, p in zip(neg, pos):\n",
    "        if n > p:\n",
    "            sentiments.append('Negative')\n",
    "        elif p > n:\n",
    "            sentiments.append('Positive')\n",
    "        else:\n",
    "            sentiments.append('Neutral')\n",
    "\n",
    "    combined_df[\"Sentiment\"] = sentiments\n",
    "\n",
    "    filtered_df = combined_df[combined_df[\"comment\"].str.contains(\"|\".join(category))]\n",
    "    filtered_df = combined_df[combined_df['Sentiment'] == 'Negative']\n",
    "    filtered_df = filtered_df.drop(columns=[\"Positive\", \"Negative\", \"Neutral\", \"Compound\"])\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Uso de la funci√≥n\n",
    "youtube_video_url = \"https://www.youtube.com/watch?v=GwgNS23SiXM\"\n",
    "category = ['emperor', 'love', 'best']\n",
    "filtered_df = process_youtube_comments(youtube_video_url, category)\n",
    "print(filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
